# Predicting text difficulty
## Motivation
The level of textual difficulty and readability has a crucial influence on how readers get information from written media. Categorizing texts by difficulty level may benefit readers by allowing them to know beforehand which texts may be more or less difficult to comprehend. The aspiration of the supervised learning part of this paper is to build a classification model that accurately classifies given texts into two classes: simple text and complex text. The focus of this task is to achieve a high accuracy in classification. Therefore, the performance of the classifiers are evaluated solely on accuracy.
## Data Source
The primary data source utilized was the UMich SIADS 695 Fall20: Predicting text difficulty dataset. This dataset was accessed via Kaggle.com as a downloadable comma-separated value document; the direct link to the data can be found in the references section of this document.
The main datasets for both supervised and unsupervised learning tasks were the WikiLarge_train.csv file and WikiLarge_Test.csv file. The WikiLarge_Train.csv file includes texts and labels of the texts. The texts are labeled 0 if the text is simple, and 1 if the text is complex. The WikiLarge_Test.csv file includes texts only. For the supervised learning task, the WikiLarge_Train.csv dataset was used for training the models.
In addition to the primary dataset, a number of other files provided in the Kaggle dataset were utilized to extract features for the classification task. A list of basic English words from Dale Chall word list (dale_chall.txt), the Age of Acquisition dataset (AoA_51715_words.csv), and Concreteness rating data (Concreteness_ratings_Brysbaert_et_al_BRM.txt) were used for feature engineering in the supervised learning task.
## Methods & Evaluation
### Data Preparation
After loading the WikiLarge_train.csv file, the dataset was shuffled and split into three sets: training, development, and test set at 70%, 20%, and 10%, respectively. The training dataset was used for feature extraction and model training, and the development dataset was used for model selection and hyperparameter tuning. However, the test set split from WikiLarge_train.csv was never utilized, as the WikiLarge_test.csv dataset was used instead to get the final accuracy of the model on Kaggle competition.
### Feature Engineering
A number of features were created to optimize the classification. A total of 46 numeric features, including 35 Part-of-speech (Pos) tag features, were extracted from the given texts. The features are listed and described in Table 1.
https://raw.githubusercontent.com/{github_user_name}/{repo_name}/{branch}/.github/images/{695_table_1}.{png}
The Dale Chall word list (dale_chall.txt) was utilized to extract the Number of words in the Dale Chall list (dale) feature. The dale feature is calculated as the ratio of the number of words that are in the Dale Chall list to the total number of words in the text after stop words are removed. This feature can be interpreted as the ratio of simpler words to total words in the text. The mean concreteness rating (Conc.M) and the 3 percentage of people who knew the word (Percent_known)from Concreteness_ratings_Brysbaert_et_al_BRM.txt was used to extract Number of words with high concreteness score (conc) and Number of words known by less people (unknown_words). The lists of words scored lower than one standard deviation from the mean in Conc.M and Percent_known were obtained and used to identify more complex and abstract words used in the texts. The AoA score from AoA_51715_words.csv file was used to extract the Number of words with high AoA score ( high_aoa) feature. As with the conc feature, a list of words with AoA Kup scores one standard deviation higher than the mean score were used to identify more difficult words in the texts.
In order to estimate the relevance of the features to the label, the Pearson correlation coefficient was calculated between the features and the label. Figure 1 shows that the number of words (word_count) has the largest positive correlation with the label, which indicates that complex texts tend to include more words. Also, the chart shows that words with Pos tag NN (singular noun), JJ (adjective), and NNP (proper noun) appear more in complex texts. Additionally, a more frequent usage of commas was found in complex texts.
(Insert figure 1)
On the other side of the spectrum, syllable_1, the number of one syllable words, has the strongest negative correlation coefficient to the label, indicating simple texts contain more one syllable words. In addition, personal pronouns (PRP) were more frequently used in simple texts. Interestingly, words with more than three syllables (syllable_more_than_3) appear to correlate negatively with the label, meaning simpler texts contain words with more than three syllables. Upon further investigation, we found that some of the simple texts consist of one word with more than three syllables, such as ‘International’, ‘Characteristics’.
In addition to the features mentioned above, the given texts were vectorized to convert into numeric values in two ways, tfidf vectorization and count vectorization, using sklearn’s TfidfVectorizer and 4 CountVectorizer. The vectorized texts were assessed with different models to be included as the final feature.
## Model Selection and Evaluation
Initially, three different classifiers were selected for exploration: logistic regression, support vector machine (SVM), and random forest classifier. First, the accuracy of each classification model was evaluated with the features created from the given text. The random forest classifier had an accuracy score of 0.72, and both the SVM classifier and logistic regression classifier achieved an accuracy score of 0.66. Although the SVM classifier had slightly higher accuracy than the logistic regression model, the logistic regression model was selected for further investigation, as training SVM takes much longer than the logistic regression model with similar performance.
Table 2 compares the logistic regression model and the random forest model. Overall, the random forest classifier performed better than the logistic regression model, so the random forest classifier was selected for the final model. Interestingly, the accuracy of the classifier was higher without tfidf vectorized text, so we continued hyperparameter tuning on random forest classifiers with, and without tfidf vectorized text feature.
(Insert Table 2)
Due to the large size of the data and the number of features, training random forest classifiers took too long. For this reason, we focused on investigating n_estimators, the number of trees used, of the classifier. Random forest classifiers employ multiple decision trees, and the n_estimators parameter decides the number of trees used in the random forest classifier. In general, models with larger n_estimators perform better, but are at an increased risk of overfitting.
The random forest classifiers were trained with n_estimators of 100, 300, 500 and 700. Overall, the accuracy was slightly higher when using only the features created. The accuracy peaked at 0.7244 and 0.7183 (random forest classifier with features only, and features and tfidf vectorized text, respectively) at 500 estimators. After peaking at 500 estimators, however, we observed a decrease in accuracy, most likely due to overfitting.
(Insert Figure 2)
As a result, a random forest classifier with an n_estimators of 500 was selected to train on the entire training dataset. We trained the classifier with and without tfidf vectorized text, and the final evaluation was conducted on the test dataset on Kaggle competition. Surprisingly, the accuracy of both classifiers were significantly higher than the accuracy we measured on the development set, scoring 0.7836 and 0.7613 (classifier with, and without tfidf features, respectively) after being fitted with the entire training dataset provided. Furthermore, the accuracy of the classifier with tfidf features was higher than the classifier without tfidf features.
## Failure Analysis
We intended to evaluate more classifiers and conduct a grid search of multiple hyperparameters using cross-validation. However, due to limited time and available computational power, only a small number of classifiers and hyperparameters were examined. Also, we were unable to derive the full benefit from the additional datasets provided. Only about 25% of the concreteness rating and AoA datasets were used to extract features.